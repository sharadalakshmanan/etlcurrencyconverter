# -*- coding: utf-8 -*-
"""ETL.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/12LZ5Qtlt7pYBxqH_OW2C21WzZENvcyJJ
"""

import datetime
def log_progress(message):
  timestamp_format = '%Y-%h-%d-%H:%M:%S' # Year-Monthname-Day-Hour-Minute-Second
  print(message+":"+str(datetime.datetime.now().strftime(timestamp_format)))





log_progress('Extract Started at')

import requests
from bs4 import BeautifulSoup
from io import StringIO
import pandas as pd
table_attribs = ["Rank","Bank_name", "GDP_USD_millions"]
def extract(url,table_attribs):
    ''' This function aims to extract the required
    information from the website and save it to a data frame. The
    function returns the data frame for further processing. '''
    url="https://en.wikipedia.org/wiki/List_of_largest_banks"
    log_progress('Extract Started at')
    try:
      response = requests.get(url)
      response.raise_for_status()
      if response.status_code == 200:
        soup = BeautifulSoup(response.text, 'html.parser')
    except requests.exceptions.RequestException as e:
       print(f"Error fetching the webpage: {e}")
    try:
       # Step 3.1: Parse the HTML content with BeautifulSoup.
      soup = BeautifulSoup(response.content, "html.parser")
      # Step 3.2: Find the specific table (usually the first table under this section).
      table = soup.find_all("table", {"class": "wikitable"})[0]
      # Step 3.3: Read the table into a DataFrame using pandas.
      df = pd.read_html(StringIO(str(table)))[0]
    except Exception as e:
       print(f"Error parsing the HTML or reading the table: {e}")
    log_progress('Extract Completed at')
    return df

import pandas as pd
import numpy as np
#exchange_rate['GBP']=0.732398
exchange_rate=pd.DataFrame([['GBP',73.2],['EUR',.97],['INR',85.77]],columns=['Currency','Rate'])
GBP_VAL=exchange_rate.where(exchange_rate.Currency=='GBP').dropna().Rate.to_frame()
EUR_VAL=exchange_rate.where(exchange_rate.Currency=='EUR').dropna().Rate.to_frame()
INR_VAL=exchange_rate.where(exchange_rate.Currency=='INR').dropna().Rate.to_frame()
#print(exchange_rate)
print(GBP_VAL['Rate'])
print(EUR_VAL['Rate'])
print(INR_VAL['Rate'])

exchange_rate.where(exchange_rate.Currency=='INR').dropna().Rate.to_frame()

extract('https://en.wikipedia.org/wiki/List_of_largest_banks',table_attribs)
df=extract('https://en.wikipedia.org/wiki/List_of_largest_banks',table_attribs)
print(df.columns)

df.rename(columns={'Rank':'Rank','Bank name':'Bank_name','Total assets (2023) (US$ billion)':'GDP_USD_millions'},inplace=True)
print(df.columns)
print(df)

def transform(df,exchange_rate):
  log_progress('Transform Started at')
  #df.columns=['Rank','Name', 'MC_USD_Billion','MC_GBP_Billion','MC_EUR_Billion','MC_INR_Billion']
  df.rename(columns={'Rank':'Rank','Bank name':'Bank_name','Total assets (2023) (US$ billion)':'GDP_USD_millions'},inplace=True)
  print(df)
  df.head()
  df['MC_GBP_Billion'] = [np.round(x*GBP_VAL) for x in df['GDP_USD_millions']]
  df['MC_EUR_Billion'] = [np.round(x*EUR_VAL) for x in df['GDP_USD_millions']]
  df['MC_INR_Billion'] = [np.round(x*INR_VAL) for x in df['GDP_USD_millions']]
  print(df)
  log_progress('Transform Completed at')

#print(df['GDP_USD_millions'])
df['MC_GBP_Billion'] = [np.round(x*GBP_VAL) for x in df['GDP_USD_millions']]

print(df['MC_GBP_Billion'])

transform(df,exchange_rate)

import sqlite3
table_attribs = ["Bank", "GDP_USD_millions"]
db_name = 'World_Economies.db'
table_name = 'Countries_by_GDP'
csv_path = './Countries_by_GDP.csv'
sql_connection = sqlite3.connect('World_Economies.db')
def load_to_db(df, sql_connection, table_name):
  log_progress('Loading to db Started at')
  db_name='World_Economies.db'
  conn = sqlite3.connect(db_name)
  df.to_sql(table_name, conn, if_exists='replace', index=False)
  log_progress('Loading to db Completed at')
  conn.close()

load_to_db(df=df, sql_connection=sql_connection, table_name=table_name)

df.to_sql(table_name, sql_connection, if_exists = 'replace', index = False)

def load_to_csv(df, csv_path):
  log_progress('Loading to csv Started at')
  df.to_csv(csv_path)
  log_progress('Loading to csv Completed at')

def run_query(query_statement, sql_connection):
    print(query_statement)
    query_output = pd.read_sql(query_statement, sql_connection)
    print(query_output)

import requests
from bs4 import BeautifulSoup
from io import StringIO
import pandas as pd
url="https://en.wikipedia.org/wiki/List_of_largest_banks"
try:
  response = requests.get(url)
  response.raise_for_status()
  if response.status_code == 200:
    soup = BeautifulSoup(response.text, 'html.parser')
except requests.exceptions.RequestException as e:
       print(f"Error fetching the webpage: {e}")

try:
       # Step 3.1: Parse the HTML content with BeautifulSoup.
       soup = BeautifulSoup(response.content, "html.parser")

       # Step 3.2: Find the specific table (usually the first table under this section).
       table = soup.find_all("table", {"class": "wikitable"})[0]

       # Step 3.3: Read the table into a DataFrame using pandas.
       df = pd.read_html(StringIO(str(table)))[0]
except Exception as e:
       print(f"Error parsing the HTML or reading the table: {e}")


print(df)

df['MC_EUR_Billion'][4]

import requests
response = requests.get('https://en.wikipedia.org/wiki/List_of_largest_banks')
print(response)

